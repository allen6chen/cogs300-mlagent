Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
450000,3.4053655,26.250189,359.64267,0.02387537,0.00028686682,0.19562227,0.00047854916,1202.888888888889,1087.4964910488989,1087.4964910488989,1.0
500000,3.4223282,28.320517,338.68732,0.027066162,0.00028562776,0.19520926,0.0004765253,1202.3478260869565,1084.2797145699353,1084.2797145699353,1.0
550000,3.314748,32.210716,365.49994,0.025013795,0.0002842347,0.19474491,0.00047424997,1202.421052631579,1135.5917743907164,1135.5917743907164,1.0
600000,3.127983,38.23423,348.40002,0.024058256,0.00028284552,0.19428182,0.00047198092,1202.2857142857142,1261.8085254278212,1261.8085254278212,1.0
650000,3.0655086,45.589893,245.14978,0.02364001,0.00028130267,0.19376755,0.00046946103,1202.2857142857142,1166.7424112888318,1166.7424112888318,1.0
700000,2.999724,51.568756,200.4703,0.026532263,0.0002797581,0.19325268,0.0004669382,1202.35,1190.7107579892502,1190.7107579892502,1.0
750000,3.0569696,54.95788,166.01204,0.023997592,0.00027820977,0.19273658,0.0004644093,1202.3333333333333,1121.7797183131888,1121.7797183131888,1.0
800000,2.967205,59.295044,148.24773,0.025207693,0.00027666608,0.19222204,0.0004618879,1202.3809523809523,1092.2227057518348,1092.2227057518348,1.0
