Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,4.189812,-6.1524053,673.328125,-231.15395188925322,-231.15395188925322,142.84186,0.024382595,0.00029922486,0.19974165,0.00049873395,1.0
100000,4.121587,-8.618337,647.2125,-228.92371897790582,-228.92371897790582,143.16629,0.025757685,0.00029783192,0.1992773,0.00049645884,1.0
150000,4.0873694,-9.999564,651.2702702702703,-226.63670946234788,-226.63670946234788,136.62874,0.025047576,0.00029628788,0.19876261,0.0004939369,1.0
200000,4.0484433,-11.233177,655.2285714285714,-234.14570503226585,-234.14570503226585,121.265335,0.024993319,0.0002947416,0.1982472,0.00049141125,1.0
250000,4.0529757,-12.243229,801.469696969697,-289.7613973559981,-289.7613973559981,199.69089,0.024416687,0.00029319234,0.19773081,0.00048888085,1.0
300000,4.020476,-13.811119,764.0166666666667,-271.9126492838065,-271.9126492838065,200.72346,0.023965947,0.00029164628,0.19721542,0.0004863556,1.0
350000,3.989205,-14.575495,792.1290322580645,-284.7689105416789,-284.7689105416789,158.5416,0.023267306,0.00029025224,0.19675076,0.00048407863,1.0
400000,3.9444673,-15.636624,827.7586206896551,-288.000036467509,-288.000036467509,197.3769,0.024079394,0.00028885913,0.19628638,0.00048180326,1.0
