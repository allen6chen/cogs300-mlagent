Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
2050000,1.7034882,3.1176345,0.061514515,0.021739593,0.00023895375,0.17965126,0.0004002911,1202.888888888889,39.39535074432691,39.39535074432691,1.0
2100000,1.845821,2.905051,0.037634876,0.024668407,0.00023771476,0.17923824,0.00039826735,1202.3478260869565,37.85495701773713,37.85495701773713,1.0
2150000,1.6914206,2.875109,0.3044513,0.026350101,0.00023616428,0.17872141,0.00039573494,1202.421052631579,31.646207171835396,31.646207171835396,1.0
2200000,1.5562568,2.8764203,0.036536217,0.026922261,0.00023462018,0.17820671,0.00039321286,1202.3333333333333,37.65926944428966,37.65926944428966,1.0
2250000,1.57386,2.955241,0.7656676,0.025754672,0.00023323126,0.17774373,0.00039094427,1202.2222222222222,30.42526982068306,30.42526982068306,1.0
2300000,1.4554609,2.8546093,6.3263016,0.025745064,0.0002318412,0.1772804,0.00038867389,1202.35,25.935897665843367,25.935897665843367,1.0
2350000,1.2535222,2.9243789,0.84485036,0.024222571,0.00023029873,0.17676623,0.00038615454,1202.3333333333333,29.609860184646788,29.609860184646788,1.0
2400000,1.1541641,2.9747,0.18164827,0.025118127,0.00022875366,0.1762512,0.0003836309,1202.3809523809523,35.469774414740854,35.469774414740854,1.0
2450000,1.2143382,2.8296225,0.7996724,0.04491856,0.00022720997,0.17573664,0.0003811095,1202.3333333333333,28.08294906130149,28.08294906130149,1.0
