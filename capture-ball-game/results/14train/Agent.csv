Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
350000,3.5416439,9.86305,2.707633,0.026464693,0.00029013611,0.19671205,0.000483889,1202.5,98.80283778999001,98.80283778999001,1.0
400000,3.3411503,9.411127,1.1040188,0.027061027,0.00028874184,0.1962473,0.00048161167,1202.4,114.72560419775546,114.72560419775546,1.0
450000,3.2624516,9.292713,1.0904795,0.025239926,0.00028719523,0.19573176,0.00047908552,1202.3636363636363,112.55800052325834,112.55800052325834,1.0
500000,3.119455,9.8878765,0.7400533,0.025146926,0.00028564926,0.19521643,0.0004765604,1202.4,125.1919264279698,125.1919264279698,1.0
550000,3.0654154,9.990978,1.5952902,0.02596584,0.00028425502,0.19475168,0.00047428312,1202.3333333333333,106.55648048349063,106.55648048349063,1.0
600000,3.1012497,9.597193,1.5509657,0.024690427,0.0002828668,0.19428892,0.00047201576,1202.25,107.02679973170162,107.02679973170162,1.0
650000,3.0792332,9.44836,1.5193716,0.023036893,0.00028132353,0.19377449,0.00046949513,1202.3333333333333,117.13145290811856,117.13145290811856,1.0
700000,3.0403957,9.789139,1.3106439,0.024849784,0.0002797823,0.19326074,0.0004669777,1202.3636363636363,118.74371657655998,118.74371657655998,1.0
750000,2.9488404,9.922091,1.6222283,0.025196588,0.0002782385,0.19274616,0.0004644561,1202.35,110.33402476385236,110.33402476385236,1.0
